\documentclass[11pt]{article}

\input{header}

\usepackage{algorithm,algorithmic}

% VARIABLES

\newcommand{\lecturenum}{5}
\newcommand{\lecturetopic}{Indistinguishability, Pseudorandomness}
\newcommand{\scribename}{Abhinav Shantanam}

% END OF VARIABLES

\lecheader

\pagestyle{plain}               % default: no special header

\begin{document}

\thispagestyle{fancy} % first page should have special header

% LECTURE MATERIAL STARTS HERE

\section{Indistinguishability}
\label{sec:indistinguishability}

We'll now start a major unit on \emph{indistinguishability} and
\emph{pseudorandomness}.  These concepts are a cornerstone of modern
cryptography, underlying several foundational applications such as
pseudorandom generators, secure encryption, ``commitment'' schemes,
and much more.

For example, our most immediate application of indistinguishability
will be to construct cryptographically strong \emph{pseudorandom bit
  generators}.  These are algorithms that produce many
``random-looking'' bits, while using very little ``true'' randomness.
(In particular, the bits they output will necessarily be ``very far''
from truly random, in a statistical sense.)  One easy-to-imagine
application would be to use the pseudorandom bit stream as a one-time
encryption pad, which would allow the shared secret key to be much
smaller than the message.  But what does it \emph{mean} for a string
of bits to be ``random-looking''?  And how can we be confident that
using such bits does not introduce any unforeseen weaknesses in our
system?

More generally, the motivating question for our study is:
\begin{center}
  When can two (possibly different) objects be considered
  \emph{effectively the same}?
\end{center}
The answer:
\begin{center}
  \emph{When they can't be told apart!}
\end{center}
Though seemingly glib, this answer encapsulates a very powerful
mindset that will serve us well as we go forward.

\subsection{Statistical Indistinguishability}
\label{sec:stat-indist}

We use probability theory to model (in)distinguishability.  If two
distributions are identical, then they certainly should be considered
indistinguishable.  We relax this condition to define
\emph{statistical} indistinguishability, for when the
\emph{statistical distance} between the two distributions is
negligible.  The statistical distance between two distributions $X$
and $Y$ over a domain $\Omega$ is defined as\footnote{For a set~$S$ of
  real numbers, its \emph{supremum} $\sup(S)$ is the least upper bound
  of the elements of~$S$. The suprenum can be thought of as a
  generalization of the maximum element to (infinite) sets where no
  such element may exist. For example, there is no maximum element of
  the set $[0,1) \subseteq \R$, but the suprenum is 1 since it is the
  smallest real number that is larger than every element of
  $[0,1)$. When~$S$ is finite, the supremum is simply the maximum.}
\[ \Delta(X,Y) := \sup_{A \subseteq \Omega} \abs{X(A) - Y(A)}, \]
where $X(A) = \sum_{w \in A} \Pr[X=w]$ is the probability that a draw
from~$X$ lands in~$A$, and likewise for~$Y(A)$. We can
view~$A \subseteq \Omega$ as a statistical ``test'' that has some
probability of ``passing'' when given an element drawn from~$X$, or
from~$Y$; the statistical distance is essentially the maximum
difference between these two probabilities, taken over all tests.
Note that $A$ and $\bar{A}$ are effectively the same test, since
\[ \abs{X(\bar{A})-Y(\bar{A})} = \abs{1-X(A)-(1-Y(A))} =
  \abs{Y(A)-X(A)} = \abs{X(A)-Y(A)}. \]

\begin{lemma}
  \label{lem:stat-dist}
  For distributions $X,Y$ over a finite domain $\Omega$,
  \[ \Delta(X,Y) = \frac12 \sum_{w \in \Omega} \abs{X(w) - Y(w)}. \]
\end{lemma}

\begin{proof}
  Let the test $A = \set{w \in \Omega\; :\; X(w) > Y(w)}$.  This makes
  $X(A) - Y(A)$ as large as possible, so
  $\Delta(X,Y) = X(A)-Y(A) = \sum_{w \in A} \abs{X(w)-Y(w)}$.  As
  noted above, we also have
  $\Delta(X,Y) = Y(\bar{A})-X(\bar{A}) = \sum_{w \in \bar{A}}
  \abs{X(w)-Y(w)}$.  Summing the two equations, we have
  $2\Delta(X,Y) = \sum_{w \in \Omega} \abs{X(w)-Y(w)}$, as desired.
\end{proof}

Statistical distance is very robust, which enhances its usefulness.
Using \cref{lem:stat-dist}, the following facts are
straightforward to prove.

\begin{lemma}
  \label{lem:ipl}
  Let $f$ be a function (or more generally, randomized procedure) on
  the domain of $X,Y$. Then $\Delta(f(X), f(Y)) \leq \Delta(X,Y)$.
\end{lemma}
In other words, statistical distance cannot be increased by the
application of any (randomized) procedure.

\begin{lemma}
  Statistical distance is a metric; in particular, it satisfies the
  following three properties:
  \begin{itemize}
  \item Identity of indiscernibles: $\Delta(X,Y) = 0 \iff X = Y$
  \item Symmetry: $\Delta(X,Y) = \Delta(Y,X)$
  \item Subadditivity (triangle inequality):
    $\Delta(X,Z) \leq \Delta(X,Y) + \Delta(Y, Z)$
  \end{itemize}
\end{lemma}

\begin{question}
  Justify why it is the case that $\Delta(X,Y) = 0 \iff X = Y$.
\end{question}

\begin{answer}
  In the forward direction, because $\Delta(X,Y) = 0$ and all the
  terms in the sum from \cref{lem:stat-dist} are non-negative,
  we have that $\abs{X(w) - Y(w)} = 0$ for all $w \in \Omega$, i.e.,
  $X(w) = Y(w)$. Hence, $X = Y$. In the other direction, if $X = Y$,
  then by definition $X(A) - Y(A) = 0$ for all $A$, so
  $\Delta(X,Y) = \sup(\set{0}) = 0$.
\end{answer}

Statistical distance lets us say when two (sequences of) distributions
are ``essentially the same,'' in an asymptotic sense.

\begin{definition}
  \label{def:stat-ind}
  Let $\calX = \set{X_{n}}_{n \in \N}$ and
  $\calY = \set{Y_{n}}_{n \in \N}$ be sequences of probability
  distributions, called \emph{ensembles}.  We say that $\calX$ and
  $\calY$ are \emph{statistically indistinguishable}, written
  $\calX \statind \calY$, if \[ \Delta(X_{n}, Y_{n}) = \negl(n). \]
\end{definition}

\begin{example}
  Let $X_{n}$ be the uniform distribution over $\bit^{n}$, and let
  $Y_{n}$ be the uniform distribution over the nonzero strings
  $\bit^{n} \backslash \set{0^{n}}$.  An optimal test $A$ is the
  singleton set $A = \set{0^{n}}$, yielding
  $\Delta(X_{n}, Y_{n}) = 2^{-n} = \negl(n)$, so
  $\calX \statind \calY$.  (This can also be seen by calculating the
  summation in \cref{lem:stat-dist}.)  The analysis extends
  similarly to any $Y_{n}$ that leaves out a $\negl(n)$ fraction of
  $\bit^{n}$.  From this we can say that such ensembles $\calY$ are
  ``essentially uniform,'' or \emph{statistically pseudorandom}.
\end{example}

Can a statistically peudorandom generator exist?  This
depends on the definition of ``generator'' (which we give below), but
for any meaningful definition of the term, it isn't possible!  This
can be shown by explicitly demonstrating an appropriate subset (or
test) that distinguishes strings output by the generator from
uniformly random ones; see \cref{ques:3} below.

\subsection{Computational Indistinguishability}
\label{sec:comp-indist}

We can define a natural analogue of statistical distance in the
computational setting, where the ``test'' is implemented by an
\emph{efficient algorithm}.  Namely, for distributions $X$ and $Y$ and
an algorithm $\Adv$ (possibly randomized), define $\Adv$'s
\emph{distinguishing advantage} between $X$ and $Y$ as
\[ \advan_{X,Y}(\Adv) = \abs*{\Pr[\Adv(X)=1] - \Pr[\Adv(Y)=1]}. \]
(The output of $\Adv$ can be arbitrary, but we interpret $1$ as a
special output indicating that the test implemented by $\Adv$ is
``satisfied,'' and any other output as ``not satisfied.'')  We extend
this to ensembles $\calX$ and $\calY$, making
$\advan_{\calX,\calY}(\Adv)$ a function of $n \in \N$.

\begin{definition}
  \label{def:comp-ind}
  Let $\calX = \set{X_{n}}$ and $\calY = \set{Y_{n}}$ be ensembles,
  where $X_{n}$ and $Y_{n}$ are distributions over $\bit^{l(n)}$ for
  $l(n) = \poly(n)$.  We say that $\calX$ and $\calY$ are
  \emph{computationally indistinguishable}, written
  $\calX \compind \calY$, if $\advan_{\calX,\calY}(\Adv) = \negl(n)$
  for all non-uniform PPT algorithms $\Adv$.  We say that $\calX$ is
  (computationally) \emph{pseudorandom} if
  $\calX \compind \set{U_{l(n)}}$, the ensemble of uniform
  distributions over $\bit^{l(n)}$.
\end{definition}

The basic facts about statistical distance also carry over to computational indistinguishability, where we restrict all functions/tests to be \emph{efficient}.
We call the analogue of \cref{lem:ipl} the \emph{composition lemma}, because it involves the composition of an applied procedure and a distinguisher.
We call the analogue of the triangle inequality the \emph{hybrid lemma}, because (as we will see soon) we usually invoke it on a sequence of ``hybrid'' distributions that interpolate step-by-step between two distributions we ultimately care about.

\begin{lemma}[Composition lemma]
  \label{lem:composition}
  Let $\calX = \set{X_{n}}$ and $\calY = \set{Y_{n}}$ be ensembles, $\Sim$ be any non-uniform PPT algorithm, and $\calX' = \set{X'_{n} = \Sim(X_{n})}$ and $\calY' = \set{Y'_{n} = \Sim(Y_{n})}$.
  Then for any non-uniform PPT algorithm~$\Dist'$, we have that $\advan_{\calX',\calY'}(\Dist') = \advan_{\calX,\calY}(\Dist' \circ \Sim)$.
  In particular, if $\calX \compind \calY$, then $\calX' \compind \calY'$.
\end{lemma}

\begin{proof}
  Consider the composed algorithm $\Dist = \Dist' \circ \Sim$, i.e., on input~$x$, it runs $\Dist(\Sim(x))$ and outputs the same answer.
  By construction, we have that
  \[
    \begin{aligned}
      \advan_{\calX',\calY'}(\Dist')
      &= \abs{\Pr[\Dist'(X'_{n})=1] - \Pr[\Dist'(Y'_{n})=1]} \\
      &= \abs{\Pr[\Dist'(\Sim(X_{n}))=1] - \Pr[\Dist'(\Sim(Y_{n}))=1]} \\
      &= \advan_{\calX,\calY}(\Dist) ,
    \end{aligned}
  \]
  as claimed.
  In particular, if $\calX \compind \calY$, then because~$\calD$ is non-uniform PPT by construction, the right-hand side is $\negl(n)$, hence so is the left-hand side.
\end{proof}

\begin{lemma}[Hybrid lemma]
  \label{lem:hybrid}
  Let $X^{i}$ for $i \in \set{0, 1, \ldots, m}$ be distributions for some positive integer~$m$.
  Then for \emph{any} algorithm~$\Dist$ (regardless of efficiency),
  \[ \advan_{X^{0},X^{m}}(\Dist) \leq \sum_{i \in [m]} \advan_{X^{i-1}, X^{i}}(\Dist) .
  \]
  In particular, let $\calX^{i} = \set{X^{i}_{n}}$ be~$m$ ensembles, where~$m$ is a constant independent of~$n$.
  If $\calX^{i-1} \compind \calX^{i}$ for every $i \in [m]$, then $\calX^{0} \compind \calX^{m}$.
\end{lemma}

\begin{proof}
  For each~$i$, let $p_{i} = \Pr[\Dist(X^{i}) = 1]$.
  By the triangle inequality, we can write $\Dist$'s advantage as
  \[ \advan_{X^{0}, X^{m}}(\Dist) = \abs{p_{0} - p_{m}} \leq \sum_{i \in [m]} \abs{p_{i-1} - p_{i}} = \sum_{i \in [m]} \advan_{X^{i-1},X^{i}}(\Dist) .
  \]
  
  Now consider the ensembles $\calX^{i}$, and restrict to non-uniform PPT~$\Dist$.
  By assumption, each $\advan_{\calX^{i-1}, \calX^{i}}(\Dist) = \nu_{i}(n)$, where $\nu_{i}(n)$ is a negligible function that may be \emph{different for each $i \in [m]$}.
  Fortunately, the sum of any \emph{constant} number of negligible functions is indeed negligible: letting $\nu(n) = \sum_{i} \nu_{i}(n)$, we need to show that for all $c > 0$, there exists some constant~$n_{0}$ such that $\nu(n) \leq n^{-c}$ for all $n \geq n_{0}$.
  We know that for each $i \in [m]$, there is a constant~$n_{i}$ such that $\nu_{i}(n) \leq n^{-(c+1)} \leq n^{-c}/m$ for all $n \geq n_{i}$.
  Letting $n_{0} = \max_{i \in [m]} n_{i}$ be the largest of these---which is a constant (independent of~$n$) because~$m$ is constant---it follows that $\nu(n) \leq n^{-c}$ for all $n \geq n_{0}$, as desired.
\end{proof}

\begin{remark}
  In the proof of the lemma, we used the triangle inequality on the quantities $\advan_{X^{i-1},X^{i}}(\Dist)$ to conclude something about $\advan_{X^{0},X^{m}}(\Dist)$.
  Syntactically this is unremarkable, but observe closely what we have done: even though $\Dist$'s ``goal in life'' is to distinguish between~$X^{0}$ and~$X^{m}$, by referring to the quantities $\advan_{X^{i-1},X^{i}}(\Dist)$, we are implicitly considering how $\Dist$ behaves on \emph{all} the hybrid distributions~$X^{i}$---even ones on which $\Dist$ was never ``designed'' to run!
  Yet because~$\Dist$ is ``just an algorithm,'' we can run it and use it for whatever purposes we like.
  The hybrid lemma says that in order for~$\Dist$ to distinguish (with non-negligible advantage) between the ensembles~$\calX^{0}$ and~$\calX^{m}$, it must also distinguish between~$\calX^{i}$ and~$\calX^{i+1}$ for some $i$, which is impossible by hypothesis.
\end{remark}

Importantly, the asymptotic part of \cref{lem:hybrid} considers only a \emph{constant} number of hybrid ensembles, because it critically relies on the fact that the sum of a constant number of negligible functions is negligible.
However, in many applications we will need to consider a number of hybrid distributions that \emph{grows with the security parameter}~$n$.
This is considerably more subtle: first, since the distributions in an ensemble are themselves indexed by~$n$, it is not so clear how to meaningfully define a number of ensembles that varies with~$n$.
Second, it turns out that the sum of $\poly(n)$-many \emph{different} negligible functions may be non-negligible!\footnote{Consider the sequence of functions $\nu_{i}$ for $i \in \N$, where $\nu_{i}(n) = 1$ if $n=i$, and $\nu_{i}(n)=0$ otherwise.
  Then each individual~$\nu_{i}$ is negligible, because $\nu_{i}(n) = 0$ for all $n > n_{0} := i$.
  However, defining $\nu(n) = \sum_{i=1}^{n} \nu_{i}(n)$ as the sum of the first~$n$ functions in the sequence, we have that $\nu(n) = 1$ for all~$n$, which is clearly not negligible.
  The proof of \cref{lem:hybrid} fails here because the~$n_{0}$ defined therein is not a constant, but instead depends on~$n$.}

The following corollary side-steps the first issue by considering ensembles for only the ``endpoint'' distributions, not the intermediate hybrids, and deals with the second issue by requiring that each advantage term be bounded by the \emph{same} negligible function.
This will indeed be the case in all the applications we consider, because all such terms will arise from applying the composition lemma (\cref{lem:composition}) for the \emph{same pair} of indistinguishable ``source'' ensembles $\calX,\calY$, with the \emph{same (composed) distinguisher} $\Dist = \Dist' \circ \Sim$.
(Or, slightly more generally, a constant number of indistinguishable pairs, with one composed distinguisher for each.)

\begin{corollary}
  \label{cor:hybrid-poly}
  Let $\calX = \set{X_{n}}$ and $\calY = \set{Y_{n}}$ be ensembles, and $H^{i}_{n}$ be ``hybrid'' distributions for $i \in \set{0, \ldots, m}$ where $m=\poly(n)$ may vary with~$n$, and where $H^{0}_{n} = X_{n}$ and $H^{m}_{n} = Y_{n}$.
  If $\advan_{H_{n}^{i-1},H_{n}^{i}}(\Dist) \leq \nu(n)$ for all $i \in [m]$ and a \emph{single} negligible function $\nu(n) = \negl(n)$, then $\advan_{\calX,\calY}(\Dist) = \negl(n)$ as well.
\end{corollary}

\begin{proof}
  This follows immediately from (the first part of) \cref{lem:hybrid} and the fact that $m \cdot \nu(n) = \negl(n)$, because $m = \poly(n)$.
\end{proof}

\section{Pseudorandom Generators}
\label{sec:prgs}

\begin{definition}
  \label{def:prg}
  A \emph{deterministic} function $G : \bit^{*} \to \bit^{*}$ is a
  \emph{pseudorandom generator} (PRG) with output length $\ell(n) > n$
  if:
  \begin{itemize}
  \item $G$ can be computed by a polynomial-time algorithm,
  \item $\len{G(x)} = \ell(\len{x}) > \len{x}$ for all
    $x \in \bit^{*}$, and
  \item the ensemble $\set{G(U_{n})}$ is (computationally)
    pseudorandom.
  \end{itemize}
  
  This last property essentially says that $\set{G(U_{n})}$ and
  $\set{U_{\ell(n)}}$ are computationally indistinguishable. By the
  composition lemma, it follows that $G(U_{n})$ can be used in place
  of $U_{\ell(n)}$ in \emph{any} (efficient) application!
\end{definition}

\begin{question}
  Let \(G\) be a PRG. Is \(H(x) := \overline{G(x)}\)
  \emph{necessarily} a PRG as well?  \emph{Hint:} think about the
  composition lemma.
\end{question}
  
\begin{answer}
  Yes. The intuition here is that if \(G(x)\) looks random, then its
  complement \(\overline{G(x)}\) should look like the complement of a
  uniformly random string, which itself is uniformly random.

  The formal proof is a straightforward application of the composition
  lemma. Since \(G\) is a PRG, we have
  \(G(U_n) \compind U_{\ell(n)}\), where $\ell(n)$ is $G$'s
  expansion. Consider the efficient algorithm
  \(\Sim(x) := \overline{x}\). Observe that
  \(H(x) = \overline{G(x)} = \Sim(G(x))\) for any~$x$. By the
  composition lemma, we have
  \[ H(U_{n}) = \Sim(G(U_n)) \compind \Sim(U_{\ell(n)}) \equiv
    U_{\ell(n)}, \] where the last equivalence holds because any
  bijective function applied to the uniform distribution yields the
  uniform distribution. Thus, \(H(U_n) \compind U_{\ell(n)}\), as
  needed.
\end{answer}

\begin{question}
  Prove (or at least sketch a proof) that a \emph{statistically}
  pseudorandom generator cannot exist. \emph{Hint:} think about the
  sizes of $G(\bit^n)$ and $\bit^{\ell(n)}$, and define an appropriate
  statistical test (i.e., subset of $\bit^{\ell(n)}$).
\end{question}

\begin{answer}
  Since $G$ has $n$-bit seeds (of which there are $2^n$), there are at
  most $2^n$ possible outputs of $G$ (which reside in
  $\bit^{\ell(n)}$). However, $\bit^{\ell(n)}$ is significantly larger
  than this; it has $2^{\ell(n)} \geq 2^{n+1}$ elements, making it at
  least twice as big as the image of \(G\). Informally, we can think
  of our statistical test as checking whether or not its given string
  is in the image of $G$. When the string is an output of~$G$, the
  check always succeeds; when the string is uniformly random, there is
  a significant (at least $1/2$) probability that the check fails.

  More formally and in the language of statistical indistinguishably:
  let $A = G(\bit^n) \subseteq \bit^{\ell(n)}$, i.e., the image of
  $G$, as suggested above. Because~$G$ is deterministic, we have
  $\abs{A} \leq 2^{n}$. Then
  \[
    \begin{aligned}
      \abs{G(U_n)(A) - U_{\ell(n)}(A)}
      &= \abs{1 - U_{\ell(n)}(A)} \\
      &= 1 - \abs{A} \cdot 2^{-\ell(n)}  \\
      &\geq 1 - 2^{n-\ell(n)} \\
      &\geq 1 - 2^{-1} = 1/2,
    \end{aligned}
  \]
  which is (very much) non-negligible. Since we have demonstrated a
  particular~$A$ for which
  $\abs{G(U_n)(A) - U_{\ell(n)}(A)} \geq 1/2$, the suprenum over all
  possible~$A$ cannot be smaller, so
  $\Delta(G(U_n), U_{\ell(n)}) \geq 1/2$.
\end{answer}

\subsection{Expansion of a PRG}
\label{sec:properties}

From the definition, it is easy to see that the ``weakest'' PRG we
could ask for would be one that stretches its input by just 1 bit,
i.e., $\ell(n) = n+1$.  Is there an upper limit on how much a PRG can
stretch?  The following theorem says that there is (effectively)
\emph{no limit}: if you can stretch by even just 1 bit, then you can
stretch by essentially any (polynomial) amount!

\begin{theorem}
  \label{thm:expansion}
  Suppose there exists a PRG $G$ with expansion $\ell(n)=n+1$.  Then
  for any polynomial $t(\cdot) = \poly(n)$, there exists a PRG
  $G_t: \bit^n \to \bit^{t(n)}$.
\end{theorem}

\noindent
We will prove this theorem in the next lecture.

\begin{remark}
  This theorem says something extremely strong.  Observe that the
  image $\set{G_{t}(s) : s \in \bit^{n}}$ of $G_t$ is an extremely
  small fraction $2^{n-t(n)}$ of its range set $\bit^{t(n)}$.  Yet no
  computationally bounded algorithm can distinguish a random element
  from this small subset, from a truly random one over the whole
  space!
\end{remark}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
